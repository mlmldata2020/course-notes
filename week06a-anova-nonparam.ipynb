{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week06 - ANOVA, Non-parametric tests ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence intervals for variance\n",
    "Describes the how well you know the true variance.\n",
    "\n",
    "Calculating the confidence intervals for variance:\n",
    "\n",
    "$ \\frac{(N -1) s^2}{\\chi^2 _{1-\\alpha/2}} < \\sigma^2 <  \\frac{(N -1) s^2}{\\chi^2 _{\\alpha/2}} $ \n",
    "\n",
    "In this equation, the larger $\\chi^2 _{1-\\alpha/2}$ value is in the denominator of the lower bound.\n",
    "\n",
    "Note that the $\\chi^2$ values are not symmetric\n",
    "\n",
    "$ \\chi^2_{\\alpha/2} \\ne \\chi^2_{1-\\alpha/2} $\n",
    "\n",
    "In addition, both $\\chi^2$ values are both greater than zero. This ensures that both the upper and lower limits of the confidence intervals for variance (and standard deviation) are always greater than zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA ##\n",
    "\n",
    "### Recommended reading ###\n",
    "\n",
    "McDonald, J.H. 2014. Handbook of Biological Statistics (3rd ed.). Sparky House Publishing, Baltimore, Maryland. Freely available online at [www.biostathandbook.com](www.biostathandbook.com)\n",
    "\n",
    "__Analysis of variance__: test for a statistically significant difference between means of 3+ different groups\n",
    "\n",
    "* Does not tell you which group is different\n",
    "* Requires the use of __*post hoc*__ analysis to determine which means are different from each other\n",
    "\n",
    "Example:\n",
    "<img src='images/anova_example.png' width = '600'> http://www.biostathandbook.com/onewayanova.html\n",
    "\n",
    "__One-Way ANOVA__ \n",
    "Fisher's LSD *post-hoc* test used to determine which populations are different from each other.\n",
    "\n",
    "__Two-Way ANOVA__\n",
    "Data are grouped into different genotypes, within those groupings, sex is segregated. Thus two factors are varying across the examples\n",
    "\n",
    "\n",
    "\n",
    "One-Way example:\n",
    "\n",
    "* J Populations (or \"treatments\")\n",
    "* N samples per population\n",
    "* $H_0: \\mu_1 = \\mu_2 = \\mu_3 = \\mu_4 $\n",
    "* $H_a$: One mean will be different from any of the others\n",
    "\n",
    "Three different types of CTDs in a water bath, each has four different measurements\n",
    "( Does not require the same number of samples within each population\n",
    "\n",
    "Use the F-statistic: The ratio of the variances of two groups of samples taken from a normal distribution follows an *F* distribution\n",
    "\n",
    "$$ F = \\frac{s_1^2}{s_2^2} $$\n",
    "\n",
    "The F distribution can be used to test whether variances are  significantly different. In the case of ANOVA, we want to test whether the variance of differences between different groups is larger than the variance within groups.\n",
    "\n",
    "Sum of Squares Between: __SSB__ $$\\sum_{j=1}^J{N_j(\\bar{y_j}-\\bar{y})^2}$$where $\\bar{y_j}$ is the mean of each population and $\\bar{y}$ is the mean of all samples\n",
    "\n",
    "Mean Square Between: __MSB__ $$\\frac{SSB}{J-1}$$\n",
    "\n",
    "$J-1$ is the degrees of freedom in calculating MSB.\n",
    "\n",
    "Sum of Squares Within: __SSW__ $$ \\sum_{j=1}^J{\\sum_{i=1}^{N_j}}({y_{ij}} - \\bar{y}_i)^2  $$\n",
    "\n",
    "Mean Square Within: __MSW__ $$ \\frac{SSW} {(\\sum_{j=1}^J { N_j}) -J } $$\n",
    "\n",
    "$\\sum_{j=1}^J ({ N_j} -J )$ is the degrees of freedom in calculating MSW. This is the total number of samples minus the number of groups.\n",
    "\n",
    "F-Distribution: $$ F =\\frac{MSB}{MSW} $$\n",
    "\n",
    "\n",
    "The null hypothesis can be rejected if F is large. This is a one-tailed test, since small values of F do not lead to a rejection of the null hypothesis. The region of rejection is is above some critical level, which is determined by the confidence level and the degrees of freedon in the numerator and denominator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popular *post-hoc* tests ###\n",
    "\n",
    "* Fisher's LSD (least significant difference)\n",
    "\n",
    "* Tukey HSD (honest significant difference) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric vs. non-parametric statistical tests\n",
    "\n",
    "#### Parametric test\n",
    "- Based on parameters that summarize a distribution, such as mean and standard deviation\n",
    "- For example, t-tests and ANOVA assume a normal distribution of samples\n",
    "\n",
    "#### Non-parametric test\n",
    "- Advantage: No assumptions about parent population (more robust)\n",
    "- Disadvantage: Less power in situations where parametric assumptions are satisfied (more samples needed to draw conclusions at same confidence level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for normality\n",
    "\n",
    "The following figures come from a notebook on the central limit theorem and testing for normality of a distribution:\n",
    "\n",
    "https://github.com/tompc35/oceanography-notebooks/blob/master/central_limit_theorem.ipynb\n",
    "\n",
    "<img src='images/norm_dist_week3.png' width=\"500\">\n",
    "\n",
    "Blue: Sample distribution ($O_i$)<br>\n",
    "Red: Normal distribution with same mean and standard deviation, expected value ($E_i$)\n",
    "\n",
    "#### Chi squared test for normality\n",
    "\n",
    "$$ X^2 = \\sum_{i=1}^k \\frac{\\left(O_i - E_i\\right)^2}{E_i}$$\n",
    "\n",
    "Tests for goodness of fit\n",
    "\n",
    "Compare this test statistic to the Chi-Squared distribution $\\chi_{\\nu, 1-\\alpha}^2$, where $\\nu = k-1$ is the degrees of freedom.\n",
    "\n",
    "- If test statistic is larger than the Chi-square value, can reject the Null Hypothesis that they are from the same distribution. Note that this test is sensitive to bin size.\n",
    "\n",
    "#### Probability Plot\n",
    "\n",
    "<img src='images/prob_dens.png' width=\"500\">\n",
    "\n",
    "The corresponding probability plot for this distribution is shown below:\n",
    "\n",
    "<img src='images/prob_plot.png' width=\"500\">\n",
    "\n",
    "The x-axis is the _quantiles_ of the normal. If a normal distribution is split up into some discrete number of pieces, the quantiles are the z-scores at the edges of each piece. The quantiles are tightly clustered near zero.\n",
    "\n",
    "The y-axis is the _ordered values_ in the sample distribution.\n",
    "\n",
    "If values are normally distributed, the quantiles should plot linearly with the ordered values. That is, most values are clustered around the mean. Note that this test is qualitative and the $R^2$ statistic does not have much meaning in this case. As we will see later, correlation statistics are only meaningful of the variables are normally distributed.\n",
    "\n",
    "###### Example for a non-normal distribution:\n",
    "\n",
    "<img src='images/non_norm_dist.png' width=\"500\">\n",
    "<img src='images/non_norm_prob_plot.png' width=\"500\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kolmogorov-Smirnov test\n",
    "\n",
    "Can be used to compare two sample distributions, or a sample distribution and a reference distribution (normal, exponential, etc.)\n",
    "\n",
    "Null Hypothesis: Samples are drawn from the same distribution (in the two-sample case)\n",
    "\n",
    "##### An oceanographic example\n",
    "\n",
    "<img src='images/km_dist.png' width=\"500\">\n",
    "\n",
    "_Source_: Durkin et al (2009), Chitin in diatoms and its association with the cell wall, Eukaryotic Cell\n",
    "\n",
    "The following graph illustrates the K-S test statistic for a two-sample test.\n",
    "\n",
    "<img src='images/KS_wiki.png' width=\"400\">\n",
    "Source: https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test <br>\n",
    "\n",
    "Illustration of the two-sample Kolmogorovâ€“Smirnov statistic. Red and blue lines each correspond to an empirical distribution function, and the black arrow is the two-sample KS statistic.\n",
    "\n",
    "This is a very sensitive test, therefore with lots of samples it is very easy to reject the null hypothesis. i.e. low power\n",
    "\n",
    "```python\n",
    "from scipy import stats\n",
    "\n",
    "help(stats.kstest)\n",
    "```\n",
    "\n",
    "### Other tests for normality\n",
    "\n",
    "__Shapiro-Wilk__\n",
    "- High Power\n",
    "- Biased at __Large__ sample size\n",
    "\n",
    "```python\n",
    "from scipy import stats\n",
    "\n",
    "help(stats.shapiro)\n",
    "```\n",
    "\n",
    "__Anderson-Darling__\n",
    "\n",
    "These tests, along with the K-S test and probability plots are included in the Python stats library.\n",
    "\n",
    "```python\n",
    "from scipy import stats\n",
    "help(stats.anderson)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometric mean\n",
    "\n",
    "If you were to log-transform data and then do a T-test, you'd be testing for a differences between geometric means.\n",
    "\n",
    "\n",
    "Will amplify the large values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-parametric tests: univariate data\n",
    "\n",
    "#### Wilcoxan signed-rank test\n",
    "\n",
    "__$H_0$__: the median difference between pairs of observations is zero\n",
    "\n",
    "- Rank the absolute values of the differences (smallest = 1)\n",
    "- Sum the ranks of the positive values, and sum the ranks the negative values separately\n",
    "- The smaller of the two sums is the test statistic T\n",
    "- Low values of T required for significance\n",
    "- Use __Mann-Whitney__ test for unpaired data\n",
    "\n",
    "```python\n",
    "from scipy import stats\n",
    "\n",
    "stats.wilcoxon\n",
    "```\n",
    "\n",
    "#### Mann- Whitney test\n",
    "- ranked test\n",
    "- analaogue of t-test for independent samples\n",
    "\n",
    "```python\n",
    "from scipy import stats\n",
    "\n",
    "stats.mannwhitneyu\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kruskal-Wallis ANOVA\n",
    "\n",
    "__$H_0$__: Means of ranks of groups are the same <br>\n",
    "__$H_0 (II)$__: Medians of groups are the same (assuming they come from distributions with the same shape)\n",
    "\n",
    "- Related to the Mann-Whitney rank-sum test (two groups)\n",
    "- Does not assume normality, but...\n",
    "- According to [McDonald](http://www.biostathandbook.com), the Fisher's classic ANOVA is not actually very sensitive to non-normal distributions\n",
    "- Like Fisher's classic ANOVA, testing $H_0 (II)$ does not assume difference groups have same variance( homoscedasticity)\n",
    "- Welch's ANOVA is another alternative to Fisher's ANOVA that does not assume homoscedasticity (like Welch's t-test)\n",
    "\n",
    "```python\n",
    "from scipy import stats\n",
    "\n",
    "stats.kruskal\n",
    "```\n",
    "\n",
    "https://docs.scipy.org/doc/scipy-0.14.0/reference/stats.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
